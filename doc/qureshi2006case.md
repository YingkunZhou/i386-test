# A Case for MLP-Aware Cache Replacement

## Abstract:

Performance loss due to long-latency memory accesses can be reduced by servicing multiple memory accesses concurrently. The notion of generating and servicing long-latency cache misses in parallel is called memory level parallelism (MLP). MLP is not uniform across cache misses - some misses occur in isolation while some occur in parallel with other misses. Isolated misses are more costly on performance than parallel misses. However, traditional cache replacement is not aware of the MLP-dependent cost differential between different misses. Cache replacement, if made MLP-aware, can improve performance by reducing the number of performance-critical isolated misses. This paper makes two key contributions. First, it proposes a framework for MLP-aware cache replacement by using a runtime technique to compute the MLP-based cost for each cache miss. It then describes a simple cache replacement mechanism that takes both MLP-based cost and recency into account. Second, it proposes a novel, low-hardware overhead mechanism called sampling based adaptive replacement (SBAR), to dynamically choose between an MLP-aware and a traditional replacement policy, depending on which one is more effective at reducing the number of memory related stalls. Evaluations with the SPEC CPU2000 benchmarks show that MLP-aware cache replacement can improve performance by as much as 23%.

## Introduction

As the imbalance between processor and memory speeds increases, the focus on improving system performance moves to the memory system. Currently, processors are supported by large on-chip caches that try to provide faster access to recently-accessed data. Unfortunately, when there is a miss at the largest on-chip cache, instruction processing stalls after a few cycles [10], and the processing resources remain idle for hundreds of cycles [23]. The inability to process instructions in parallel with long-latency cache misses results in substantial performance loss. One way to reduce this performance loss is to process the cache misses in parallel.(<sub>Unless stated otherwise, cache refers to the largest on-chip cache. Cache miss refers to a miss in the largest on-chip cache. Multiple concurrent misses to the same cache block are treated as a single miss. Parallel miss refers to a miss that is serviced while there is at least one more miss outstanding. Isolated miss refers to a miss that is not serviced concurrently with any other miss.</sub>) Techniques such as non-blocking caches [12], out-of-order execution with large instruction windows, runahead execution [5] [15], and prefetching improve performance by parallelizing long-latency memory operations. The notion of generating and servicing multiple outstanding cache misses in parallel is called Memory Level Parallelism (MLP) [6].

## 1.1. Not All Misses are Created Equal

Servicing misses in parallel reduces the number of times the processor has to stall due to a given number of long-latency memory accesses. However, MLP is not uniform across all memory accesses in a program. Some misses occur in isolation (e.g., misses due to pointer-chasing loads), whereas some misses occur in parallel with other misses (e.g., misses due to array accesses). The performance loss resulting from a cache miss is reduced when multiple cache misses are serviced in parallel because the idle cycles waiting for memory get amortized over all the concurrent misses. Isolated misses hurt performance the most because the processor is stalled to service just a single miss. The non-uniformity in MLP and the resultant non-uniformity in the performance impact of cache misses opens up an opportunity for cache replacement policies that can take advantage of the variation in MLP. Cache replacement, if made MLP-aware, can save isolated (relatively more costly) misses instead of parallel (relatively less costly) misses.

Unfortunately, traditional cache replacement algorithms are not aware of the disparity in performance loss that results from the variation in MLP among cache misses. Traditional replacement schemes try to reduce the absolute number of misses with the implicit assumption that reduction in misses correlates with reduction in memory related stall cycles. However, due to the variation in MLP, the number of misses may or may not correlate directly with the number of memory related stall cycles. We demonstrate how ignoring MLP information in replacement decisions hurts performance with the following example. Figure 1(a) shows a loop containing 11 memory references. There are no other memory access instructions in the loop and the loop iterates many times.

Let $K(K>4)$ be the size of the instruction window of the processor on which the loop is executed. Points A, B, C, D, and E each represent an interval of at least K instructions. Between point A and point B, accesses to blocks P1, P2, P3, and P4 occur in the instruction window at the same time. If these accesses result in multiple misses then those misses are serviced in parallel, stalling the processor only once for the multiple parallel misses. Similarly, accesses between point B and point C will lead to parallel misses if there is more than one miss, stalling the processor only once for all the multiple parallel misses. Conversely, accesses to block S1, S2, or S3 result in isolated misses and the processor will be stalled once for each such miss. We analyze the behavior of this access stream for a fully-associative cache that has space for four cache blocks, assuming the processor has already executed the first iteration of the loop.

First, consider a replacement scheme which tries to minimize the absolute number of misses, without taking MLP information into account. Belady's OPT [2] provides a theoretical minimum for the number of misses by evicting a block that is accessed furthest in the future. Figure 1(b) shows the behavior of Belady's OPT for the given access stream. At point B, blocks P1, P2, P3, and P4 were accessed in the immediate past and will be accessed again in the immediate future. Therefore, the cache contains blocks P1, P2, P3, and P4 at point B. This results in hits for the next accesses to blocks P4, P3, P2, and P1, and misses for the next accesses to blocks S1, S2, and S3. To guarantee the minimum number of misses, Belady's OPT evicts P4 to store S1, S1 to store S2, and S2 to store S3. Since the misses to S1, S2, and S3 are isolated misses, the processor incurs three long-latency stalls between points C and A'. At point A, the cache contains P1, P2, P3, and S3 which results in a miss for P4, stalling the processor one more time. Thus, for each iteration of the loop, Belady's OPT causes four misses (S1, S2, S3, and P4) and four long-latency stalls.

![图片](https://user-images.githubusercontent.com/43129850/145836965-cb772e5c-cc02-4d30-8067-c5d86c0c1723.png)

Figure 1. The drawback of not including MLP information in replacement decisions.

Second, consider a simple MLP-aware policy, which tries to reduce the number of isolated misses. This policy keeps in cache the blocks that lead to isolated misses (S1, S2, S3) rather than the blocks that lead to parallel misses (P1, P2, P3, P4). Such a policy evicts the least-recently used P-block from the cache. However, if there is no P-block in the cache, then it evicts the least-recently used S-block. Figure 1(c) shows the behavior of such an MLP-aware policy for the given access stream. The cache has space for four blocks and the loop contains only 3 S-blocks (S1, S2, and S3). Therefore, the MLP-aware policy never evicts an S-block at any point in the loop. After the first loop iteration, each access to S 1, S2, and S3 results in a hit. At point A, the cache contains S1, S2, S3, and Pl. From point A to B, the access to P1 hits in the cache, and the accesses to P2, P3, and P4 miss in the cache. However, these misses are serviced in parallel, therefore the processor incurs only one long-latency stall for these three misses. The cache evicts P1 to store P2, P2 to store P3, and P3 to store P4. So, at point B, the cache contains S1, S2, S3, and P4. Between point B and point C, the access to block P4 hits in the cache, while accesses to blocks P3, P2, and P1 miss in the cache. These three misses are again serviced in parallel, which results in one long-latency stall. Thus, for each loop iteration, the MLP-aware policy causes six misses ([P2, P3, P4] and [P3, P2, P1]) and only two long-latency stalls.

Note that Belady's OPT uses oracle information, whereas the MLP-aware scheme uses only information that is available to the microarchitecture. Whether a miss is serviced in parallel with other misses can easily be detected in the memory system, and the MLP-aware replacement scheme uses this information to make replacement decisions. For the given example, even with the benefit of an oracle, Belady's OPT incurs twice as many long-latency stalls compared to a simple MLP-aware policy.(<sub>We use Belady's OPT in the example only to emphasize that the concept of reducing the number of misses and making the replacement scheme MLP-aware are independent. However, Belady's OPT is impossible to implement because it requires knowledge of the future. Therefore, we will use LRU as the baseline replacement policy for the remainder of this pa-per. For the LRU policy, each iteration of the loop shown in Figure 1 causes six misses ([P2, P3, P4], S1, S2, S3) and four long-latency stalls</sub>) This simple example demonstrates that it is important to incorporate MLP information into replacement decisions.

### 1.2. Contributions

Based on the observation that the aim of a cache replacement policy is to reduce memory related stalls, rather than to reduce the raw number of misses, we propose MLP-aware cache replacement and make the following contributions:
1. As a first step to enable MLP-aware cache replacement, we propose a run-time algorithm that can compute MLP-based cost for in-flight misses.
2. We show that, for most benchmarks, the MLP-based cost repeats for consecutive misses to individual cache blocks. Thus, the last-time MLP-based cost can be used as a predictor for the next-time MLP-based cost.
3. We propose a simple replacement policy called the Linear (LIN) policy which takes both recency and MLP-based cost into account to implement a practical MLP-aware cache replacement scheme. Evaluation with the SPEC CPU2000 benchmarks shows performance improvement of up to 23% with the LIN policy.
4. The LIN policy does not perform well for benchmarks in which the MLP-based cost differs significantly for consecutive misses to an individual cache block. We propose the Tournament Selection (TSEL) mechanism to select between LIN and LRU, on a per-set basis, depending on which policy results in the least number of memory related stall cycles.
5. It is expensive to implement the TSEL mechanism on a per-set basis for all the sets in the cache. Based on the key insight that a few sampled sets can be used to decide the replacement policy globally for the cache, we propose a novel, low-hardware-overhead adaptation mechanism called Sampling Based Adaptive Replacement (SBAR). SBAR allows dynamic selection between LIN and LRU while incurring a storage overhead of 1854B (less than 0.2% of the area of the baseline 1MB cache).

## Background

Out-of-order execution inherently improves MLP by continuing to execute instructions after a long-latency miss. Instruction processing stops only when the instruction window becomes full. If additional misses are encountered before the window becomes full, then these misses are serviced in parallel with the stalling miss. The analytical model of out-of-order superscalar processors proposed by Karkhanis and Smith [11] provides fundamental insight into how parallelism in L2 misses can reduce the cycles per instruction incurred due to L2 misses.

The effectiveness of an out-of-order engine's ability to increase MLP is limited by the instruction window size. Several proposals [15] [1] [4] [25] have looked at the problem of scaling the instruction window for out-of-order processors. Chou et al. [3] analyzed the effectiveness of different microarchitectural techniques such as out-of-order execution, value prediction, and runahead execution on increasing MLP. They concluded that microarchitecture optimizations can have a profound impact on increasing MLP. They also formally defined instantaneous MLP as the number of useful long-latency off-chip accesses outstanding when there is at least one such access outstanding. MLP can also be improved at the compiler level. Read miss clustering [17] is a compiler technique in which the compiler reorders load instructions with predictable access patterns to improve memory parallelism.

All of the techniques described thus far try to improve MLP by overlapping long-latency memory operations. MLP is not uniform across all memory accesses in a program though. While some of the misses are parallelized, many misses still occur in isolation. It makes sense to make this variation in MLP visible to the cache replacement algorithm. Cache replacement, if made MLP-aware, can increase performance by reducing the number of isolated misses at the expense of parallel misses. To our knowledge no previous research has looked at including MLP information in replacement decisions. Srinivasan et al. [22] [21] analyzed the criticality of load misses for out-of-order processors. But, criticality and MLP are two different properties. Criticality, as defined in [22], is determined by how long instruction processing continues after a load miss, whereas, MLP is determined by how many additional misses are encountered while servicing a miss.

Cost-sensitive replacement policies for on-chip caches were investigated by Jeong and Dubois [8] [9]. They proposed variations of LRU that take cost (any numerical property associated with a cache block) into account. In general, any cost-sensitive replacement scheme, including the ones proposed in [9], can be used for implementing an MLP-aware replacement policy. However, to use any cost-sensitive replacement scheme, we first need to define the cost of each cache block based on the MLP with which it was serviced. As the first step to enable MLP-aware cache replacement, we introduce a run-time technique to compute MLP-based cost.

## Computing MLP-Based Cost

For current instruction window sizes, instruction processing stalls shortly after a long-latency miss occurs. The number of cycles for which a miss stalls the processor can be approximated by the number of cycles that the miss spends waiting to get serviced. For parallel misses, the stall cycles can be divided equally among all concurrent misses.

### 3.1. Algorithm

The information about the number of in-flight misses and the number of cycles a miss is waiting to get serviced can easily be tracked by the MSHR (Miss Status Holding Register). Each miss is allocated an MSHR entry before a request to service that miss is sent to memory [12]. To compute the MLP-based cost, we add a field mlp.cost to each MSHR entry. The algorithm for calculating the MLP-based cost of a cache miss is shown in Algorithm 1.

![图片](https://user-images.githubusercontent.com/43129850/145839179-b3dce296-e35b-477a-82ec-595ba75dc201.png)

Algorithm 1 Calculate MLP-Based Cost for Cache Misses

When a miss is allocated an MSHR entry, the mlp_cost field associated with that entry is initialized to 0. We count instruction accesses, load accesses, and store accesses that miss in the largest on-chip cache as demand misses. All misses are treated on correct path until they are confirmed to be on the wrong path. Misses on the wrong path are not counted as demand misses. Each cycle, the mlp_cost of all demand misses in the MSHR is incremented by the amount I/(Number of outstanding demand misses in MSHR).(<sub>The number of adders required for the proposed algorithm is equal to the number of MSHR entries. However, for the baseline machine with 32 MSHR entries, time sharing four adders among the 32 entries has only a negligible effect on the absolute value of the MLP-based cost. For all our experiments, we assume that the MSHR contains only four adders for calculating the MLP-based cost. If more than four MSHR entries are valid, then the adders are time-shared between all the valid entries using a simple round-robin scheme.</sub>), (<sub>We also experimented by increasing the mlp_cost only during cycles when there is a full window stall. However, we did not find any significant difference in the relative value of mlp_cost or the performance improvement provided by our proposed replacement scheme. Therefore, for sim_plicity, we assume that the mlp_cost is updated every cycle.</sub>) When a miss is serviced, the mlp_cost field in the MSHR represents the MLP-based cost of that miss. Henceforth, we will use mlp-cost to denote MLP-based cost.

![图片](https://user-images.githubusercontent.com/43129850/145839777-29bf066c-cbcc-4768-91d7-c63ef076d998.png)

Figure 2. Distribution of mlp-cost. The horizontal axis represents the value of mlp-cost in cycles and the vertical axis represents the percentage of total misses. The dot on the horizontal axis represents the average value of mlp-cost.

### 3.2. Distribution of mlp-cost

Figure 2 shows the distribution of mlp-cost for 14 SPEC benchmarks measured on an eight-wide issue, out-of-order processor with a 128-entry instruction window. An isolated miss takes 444 cycles (400-cycle bank access + 44-cycle bus delay) to get ser-viced. The vertical axis represents the percentage of all misses and the horizontal axis corresponds to different values of mlp-cost. The graph is plotted with 60-cycle intervals, with the leftmost bar representing the percentage of misses that had a value of 0 ≤ mlp-cost < 60 cycles. The rightmost bar represents the percentage of all misses that had an mlp-eost of more than 420 cycles. All isolated misses (and some parallel misses that are serialized because of DRAM bank conflicts) are accounted for in the right-most bar.

For each benchmark, the average value of mlp-cost is much less than 444 cycles (number of cycles needed to serve an isolated miss). For art, more than 85% of the misses have an mlp-eost of less than 120 cycles indicating a high parallelism in misses. For mcf, about 40% of the misses have an mlp-eost between 180 and 240 cycles, which corresponds to two misses in parallel. Mcf also has about 9% of its misses as isolated misses. Facerec has two distinct peaks, one for the misses that occur in isolation and the other for the misses that occur with a parallelism of two. Twolf, vpr, facerec, and parser have a high percentage of isolated misses and hence the peak for the rightmost bar. The results for all of these benchmarks clearly indicate that there exists non-uniformity in mlp-cost which can be exploited by MLP-aware cache replacement. The objective of MLP-aware cache replacement is to reduce the number of isolated (i.e., relatively more costly) misses without substantially increasing the total number of misses. mlp-eost can serve as a useful metric in designing an MLP-aware replacement scheme. However, for the decision based on mlp-eost to be meaningful, we need a mechanism to predict the future mlp-eost of a miss given the current mlp-eost of a miss. For example, a miss that happens in isolation once can happen in parallel with other misses the next time, leading to significant variation in the mlp-cost for the miss. If mlp-cost is not predictable for a cache block, the information provided by the mlp-cost metric is not useful. The next section examines the predictability of mlp-cost.

### 3.3. Predictability of the mlp-cost metric

One way to predict the future mlp-eost value of a block is to use the current mlp-eost value of that block. The usefulness of this scheme can be evaluated by measuring the difference between the mlp-eost for successive misses to a cache block. We call the absolute difference in the value of mlp-cost for successive misses to a cache block as delta. For example, let cache block A have mlp-eost values of {444 cycles, 80 cycles, 80 cycles, 220 cycles} for the four misses it had in the program. Then, the first delta for block A is 364(∥444−80∥) cycles, the second delta for block A is O(∥80−80∥) cycles, and the third delta for block A is 140(∥80−220∥) cycles. To measure delta, we do an off-line analysis of all the misses in the program. Table 1 shows the distribution of delta. A small delta value means that mlp-cost does not significantly change between successive misses to a given cache block.

![图片](https://user-images.githubusercontent.com/43129850/145840068-84e1e4ba-8c72-4d1b-877a-ddd3319a8d85.png)

Table 1. The first three rows represent the percentage of deltas that were between 0-59 cycles, 60-119 cycles, and more than 120 cycles respectively. The last row represents the average value of delta.

For all the benchmarks, except bzip2, parser, and mgrid, the majority of the delta values are less than 60 cycles. The average delta value is also fairly low, which means that the next-time mlp-cost for a cache block remains fairly close to the current mlp-cost. Thus, the current mlp-eost can be used as a predictor of the next mlp-cost of the same block in MLP-aware cache replacement. We describe our experimental methodology before discussing the design and implementation of an MLP-aware cache replacement scheme based on these observations.

## Experimental Methodology

### 4.1. Configuration

We perform our experiments using an execution-driven simulator that models the Alpha ISA. Table 2 describes the baseline configuration. Our baseline processor is an aggressive eight-wide issue, out-of-order superscalar with a 128-entry instruction window. Because our studies deal with the memory system, we model bank conflicts, queueing delays, and port contention in detail. An isolated miss requires 444 cycles (400-cycle memory access + 44-cycle bus delay) to get serviced. Store instructions that miss in the L2 cache do not block the window unless the store buffer is full.

![图片](https://user-images.githubusercontent.com/43129850/145840347-f203efe9-d562-4c51-97ab-c9a6764e8682.png)

Table 2. Baseline processor configuration.

### 4.2. Benchmarks

We use SPEC CPU2000 benchmarks compiled for the Alpha ISA with the -fast optimizations and profiling feedback enabled. For each benchmark, a representative slice of 250M instructions was obtained with a tool we developed using the Simpoint [19] methodology. For all benchmarks, except apsi, the reference input set is used. For apsi, the train input set is used.

Because cache replacement cannot reduce compulsory misses, benchmarks that have a high percentage of compulsory misses are unlikely to benefit from improvements in cache replacement algorithms. Therefore, we show detailed results only for benchmarks where less than 50% of the misses are compulsory.(<sub>For the remaining SPEC CPU2000 benchmarks, the majority of the misses are compulsory misses. Therefore, our proposed scheme does not significantly affect the performance of these benchmarks. With the proposed MLP-aware replacement scheme, the performance improvement ranges from +2.5% to −0.5% for those benchmarks.</sub>) Table 3 shows the type, the fast-forward interval (FFWD), the number of L2 misses, and the percentage of compulsory misses for each benchmark.

![图片](https://user-images.githubusercontent.com/43129850/145841069-cfe51258-c8c4-4786-9da6-698d081ebcb4.png)

Table 3. Benchmark summary. (B = Billion)

## The Design of an MLP-Aware Cache Replacement Scheme

Figure 3(a) shows the microarchitecture design for MLP-aware cache replacement. The added structures are shaded. The cost calculation logic (CCL) contains the hardware implementation of Algorithm 1. It computes mlp-cost for all demand misses. When a miss gets serviced, the mlp-cost of the miss is stored in the tag-store entry of the corresponding cache block. For replacement, the cache invokes the Cost Aware Replacement Engine (CARE) to find the replacement victim. CARE can consist of any generic cost-sensitive scheme [9] [16]. We evaluate MLP-aware cache replacement using both an existing as well as a novel cost-sensitive replacement scheme.

![图片](https://user-images.githubusercontent.com/43129850/145841678-43fd9c60-a372-4d35-8046-7cd71f767eea.png)

Before discussing the details of the MLP-aware replacement scheme, it is useful to note that the exact value of mlp-cost is not necessary for replacement decisions. In a real implementation, to limit the storage overhead, the value of mlp-cost can be quantized to a few bits and the quantized value would be stored in the tag-store. We consider one such quantization scheme. It converts the value of mlp-cost into a 3-bit quantized value, according to the intervals shown in Figure 3(b). Henceforth, we use cost, to denote the quantized value of mlp-cost.

### 5.1. The Linear (LIN) Policy

The baseline replacement policy is LRU. The replacement function of LRU selects the candidate cache block with the least recency. Let $Victim_{LRU}t$ be the victim selected by LRU and $R(i)$ be the recency value (highest value denotes the MRU and lowest value denotes LRU) of block $i$. Then, the victim of the LRU policy can be written as:
$\Large Victim_{LRU} =\arg\mathop{\min}_{i} \{R(i)\} \eqno{\hbox{(1)}}$

